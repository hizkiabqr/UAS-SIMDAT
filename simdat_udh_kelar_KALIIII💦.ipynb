{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Library parsing\n",
        "from bs4 import BeautifulSoup\n",
        "import zipfile\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "tzQ-BMg5zuBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xml_folder = \"train/XML\"\n",
        "all_files = glob.glob(os.path.join(xml_folder, \"*.xml\"))\n",
        "article_ids = [os.path.basename(f).replace(\".xml\", \"\") for f in all_files]\n",
        "print(f\"Jumlah total file (article_id): {len(article_ids)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2ibd-rDzmfK",
        "outputId": "083ef470-df45-4722-8df0-3e2b1740d421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah total file (article_id): 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_labels = pd.read_csv(\"train_labels.csv\")\n",
        "print(df_labels.columns)\n",
        "print(df_labels.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKLPGF7x0KFF",
        "outputId": "37dfbbfb-2479-472a-c013-f431f70fa999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['article_id', 'dataset_id', 'type'], dtype='object')\n",
            "               article_id                      dataset_id     type\n",
            "0    10.1002_2017jc013030  https://doi.org/10.17882/49388  Primary\n",
            "1  10.1002_anie.201916483                         Missing  Missing\n",
            "2  10.1002_anie.202005531                         Missing  Missing\n",
            "3  10.1002_anie.202007717                         Missing  Missing\n",
            "4  10.1002_chem.201902131                         Missing  Missing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ext"
      ],
      "metadata": {
        "id": "6HKERQvZzjqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "xml_folder = \"train/XML\"\n",
        "\n",
        "primer_keywords = [\n",
        "    \"we collected\", \"interview\", \"measured\", \"generated\", \"field study\",\n",
        "    \"questionnaire\", \"conducted our study\", \"samples were collected\",\n",
        "    \"data was obtained in this study\", \"experimentally measured\",\n",
        "    \"we conducted\", \"data collected in this study\", \"our dataset\",\n",
        "    \"we gathered\", \"collected for this research\", \"generated during this study\", \"in-house dataset\",\n",
        "    \"experiment produced\", \"created by authors\", \"experimentally collected\", \"primary data\", \"study participants\", \"sampled from\", \"manually collected\",\n",
        "    \"we performed an experiment\", \"collected during our study\", \"observed during fieldwork\",\n",
        "    \"experimental data\", \"we carried out\", \"measured in the lab\"\n",
        "]\n",
        "\n",
        "sekunder_keywords = [\n",
        "    \"obtained from\", \"taken from\", \"downloaded\", \"publicly available\",\n",
        "    \"secondary data\", \"retrieved from\", \"sourced from\", \"previous study\",\n",
        "    \"archival\", \"published dataset\", \"borrowed from\", \"re-used\", \"reused\",\n",
        "    \"existing dataset\", \"external data\", \"data were accessed\",\n",
        "    \"gathered from\", \"data from previous studies\",\n",
        "    \"from online database\", \"data reuse\", \"secondary analysis\", \"available at\", \"extracted from\", \"according to previous data\",\n",
        "    \"cited from\", \"from repository\", \"already published\", \"sourced externally\", \"data citation\",\n",
        "    \"we used dataset from\", \"acquired from repository\", \"existing public dataset\", \"open data\", \"public dataset\", \"dataset from literature\",\n",
        "    \"data collected by others\", \"used existing data\", \"external database\", \"freely available\",\n",
        "    \"pre-existing data\", \"already collected\", \"downloadable dataset\", \"EMPIAR-\"\n",
        "]\n",
        "\n",
        "def extract_dataset_ids(soup):\n",
        "    dataset_ids = set()\n",
        "    for tag in soup.find_all(\"dataset_id\"):\n",
        "        if tag.text.strip():\n",
        "            dataset_ids.add(tag.text.strip())\n",
        "    for data_set in soup.find_all(\"data-set\"):\n",
        "        id_tag = data_set.find(\"id\")\n",
        "        if id_tag and id_tag.text.strip():\n",
        "            dataset_ids.add(id_tag.text.strip())\n",
        "    for ext_link in soup.find_all(\"ext-link\", {\"ext-link-type\": \"dataset\"}):\n",
        "        if ext_link.text.strip():\n",
        "            dataset_ids.add(ext_link.text.strip())\n",
        "        elif ext_link.get(\"xlink:href\"):\n",
        "            dataset_ids.add(ext_link[\"xlink:href\"].strip())\n",
        "\n",
        "    doi_pattern = r\"https?://doi\\.org/10\\.\\d{4,9}/[^\\s\\\"<>]+\"\n",
        "    semua_teks = soup.get_text(\" \")\n",
        "    dataset_ids.update(re.findall(doi_pattern, semua_teks))\n",
        "\n",
        "    special_pattern = r\"\\b(?:CHEMBL\\d+|IPR00\\d+|GSE\\d+|SRP\\d+|EMPIAR-\\d+|ENSBTAG000\\d+|IPR\\d+)\\b\"\n",
        "    dataset_ids.update(re.findall(special_pattern, semua_teks))\n",
        "\n",
        "    return list(dataset_ids)\n",
        "\n",
        "def extract_teks_dataset_id(soup, ds_id):\n",
        "    for tag in soup.find_all([\"ext-link\", \"dataset_id\"]):\n",
        "        if ds_id.lower() in str(tag).lower():\n",
        "            parent = tag.find_parent()\n",
        "            if parent:\n",
        "                return parent.get_text(separator=\" \").strip()\n",
        "\n",
        "    full_text = soup.get_text(separator=\" \")\n",
        "    idx = full_text.lower().find(ds_id.lower())\n",
        "    if idx != -1:\n",
        "        start = max(0, idx - 150)\n",
        "        end = min(len(full_text), idx + len(ds_id) + 150)\n",
        "        return full_text[start:end].strip()\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def detect_type(teks):\n",
        "    teks = teks.lower()\n",
        "    for kw in primer_keywords:\n",
        "        if kw in teks:\n",
        "            return \"Primary\"\n",
        "    for kw in sekunder_keywords:\n",
        "        if kw in teks:\n",
        "            return \"Secondary\"\n",
        "    return \"Missing\"\n",
        "\n",
        "hasil = []\n",
        "for filepath in glob.glob(os.path.join(xml_folder, \"*.xml\")):\n",
        "    article_id = os.path.basename(filepath).replace(\".xml\", \"\")\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "        soup = BeautifulSoup(file, \"xml\")\n",
        "        teks_full = soup.get_text(separator=\" \").strip()\n",
        "        dataset_ids = extract_dataset_ids(soup)\n",
        "        for ds_id in dataset_ids:\n",
        "            snippet = extract_teks_dataset_id(soup, ds_id)\n",
        "            tipe = detect_type(teks_full)\n",
        "            hasil.append({\n",
        "                \"article_id\": article_id,\n",
        "                \"dataset_id\": ds_id,\n",
        "                \"teks_dataset_id\": snippet,\n",
        "                \"type\": tipe\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(hasil).drop_duplicates()\n",
        "\n",
        "# --- Tambahkan file yang tidak punya dataset_id (Missing) ---\n",
        "all_files = glob.glob(os.path.join(xml_folder, \"*.xml\"))\n",
        "all_article_ids = {os.path.basename(f).replace(\".xml\", \"\") for f in all_files}\n",
        "extracted_article_ids = set(df['article_id'].unique())\n",
        "missing_article_ids = all_article_ids - extracted_article_ids\n",
        "missing_rows = [{'article_id': aid, 'dataset_id': 'Missing', 'type': 'Missing'} for aid in missing_article_ids]\n",
        "df = pd.concat([df, pd.DataFrame(missing_rows)], ignore_index=True)\n",
        "\n",
        "df.to_csv(\"train.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "6UWvyEBdzvep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Hapus baris Missing\n",
        "df = df[df['type'].isin(['Primary', 'Secondary'])].copy()\n",
        "\n",
        "# Tambah kolom label numerik\n",
        "df['label_num'] = df['type'].map({'Primary': 0, 'Secondary': 1})\n"
      ],
      "metadata": {
        "id": "4czacgmb4JFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tokenisasi"
      ],
      "metadata": {
        "id": "eqDfAnd42c0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "# Bersihkan teks\n",
        "def clean_teks_dataset_id(teks):\n",
        "    if isinstance(teks, str):\n",
        "        return re.sub(r\"\\s+\", \" \", teks.strip())\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "df['cleaned_teks'] = df['teks_dataset_id'].apply(clean_teks_dataset_id)\n",
        "df['label_num'] = df['type'].map({'Primary': 0, 'Secondary': 1})\n",
        "\n",
        "# Tokenisasi\n",
        "def tokenize_text(text, max_length=512):\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "df['tokenized'] = df['cleaned_teks'].apply(lambda x: tokenize_text(x))\n"
      ],
      "metadata": {
        "id": "ReskNXAJzxF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model"
      ],
      "metadata": {
        "id": "DO6i9yLJ2fC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SciBERTDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=512, is_test=False):\n",
        "        self.texts = df['cleaned_teks'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.is_test = is_test\n",
        "        if not is_test:\n",
        "            self.labels = df['label_num'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze()\n",
        "        }\n",
        "        if not self.is_test:\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n"
      ],
      "metadata": {
        "id": "_DgBHPC2qwjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_folder = \"test/XML\"\n",
        "\n",
        "hasil_test = []\n",
        "for filepath in glob.glob(os.path.join(test_folder, \"*.xml\")):\n",
        "    article_id = os.path.basename(filepath).replace(\".xml\", \"\")\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "        soup = BeautifulSoup(file, \"xml\")\n",
        "        teks_full = soup.get_text(separator=\" \").strip()\n",
        "        dataset_ids = extract_dataset_ids(soup)\n",
        "        for ds_id in dataset_ids:\n",
        "            snippet = extract_teks_dataset_id(soup, ds_id)\n",
        "            hasil_test.append({\n",
        "                \"article_id\": article_id,\n",
        "                \"dataset_id\": ds_id,\n",
        "                \"teks_dataset_id\": snippet\n",
        "            })\n",
        "\n",
        "df_test = pd.DataFrame(hasil_test).drop_duplicates()\n"
      ],
      "metadata": {
        "id": "BtMAXgHJqRi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['cleaned_teks'] = df_test['teks_dataset_id'].apply(clean_teks_dataset_id)"
      ],
      "metadata": {
        "id": "21nKAdCbqVrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umMtcJx6zp_7",
        "outputId": "555178b5-1a14-4149-fef1-17675a4b0556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['article_id', 'dataset_id', 'teks_dataset_id', 'cleaned_teks',\n",
            "       'predicted_type'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#fine tuning"
      ],
      "metadata": {
        "id": "bDPjQ14v2h0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# === 1. Split data latih & evaluasi ===\n",
        "train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df['label_num'], random_state=42)\n",
        "\n",
        "# === 2. Buat dataset untuk masing-masing ===\n",
        "train_dataset = SciBERTDataset(train_df, tokenizer, max_length=300)\n",
        "eval_dataset = SciBERTDataset(eval_df, tokenizer, max_length=300)\n",
        "\n",
        "# === 3. Hitung class weights ===\n",
        "class_weights = compute_class_weight(class_weight='balanced',\n",
        "                                     classes=np.unique(train_df['label_num']),\n",
        "                                     y=train_df['label_num'])\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(model.device)\n",
        "\n",
        "# === 4. Metrik evaluasi ===\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, preds),\n",
        "        'f1': f1_score(labels, preds, average='macro')\n",
        "    }\n",
        "\n",
        "# === 5. TrainingArguments ===\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# === 6. Custom Trainer dengan class weights ===\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import Trainer\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# === 7. Inisialisasi trainer dan training ===\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "qVSbLodtxFNQ",
        "outputId": "cc0705a8-246f-494a-e062-a8462a6d0e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\irfan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='812' max='812' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [812/812 03:08, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.010400</td>\n",
              "      <td>0.833421</td>\n",
              "      <td>0.977778</td>\n",
              "      <td>0.694340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.292388</td>\n",
              "      <td>0.987654</td>\n",
              "      <td>0.888128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.407008</td>\n",
              "      <td>0.990123</td>\n",
              "      <td>0.897468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.417219</td>\n",
              "      <td>0.990123</td>\n",
              "      <td>0.897468</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=812, training_loss=0.07981926375231269, metrics={'train_runtime': 189.3461, 'train_samples_per_second': 34.16, 'train_steps_per_second': 4.288, 'total_flos': 997149788712000.0, 'train_loss': 0.07981926375231269, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#eval"
      ],
      "metadata": {
        "id": "5xnAAXlM2lTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "tDPLqReKpNJs",
        "outputId": "cab78ab5-5807-43dd-9d40-9418759f06f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.292387992143631, 'eval_accuracy': 0.9876543209876543, 'eval_f1': 0.888127727749848, 'eval_runtime': 3.1915, 'eval_samples_per_second': 126.899, 'eval_steps_per_second': 15.98, 'epoch': 4.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#predik"
      ],
      "metadata": {
        "id": "Y_msX4w_qPyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SciBERTDataset(df_test, tokenizer, max_length=300, is_test=True)"
      ],
      "metadata": {
        "id": "z1fvkTzKqXd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 7. Prediksi pada data test ===\n",
        "predictions = trainer.predict(test_dataset)\n",
        "pred_labels = predictions.predictions.argmax(axis=-1)\n",
        "\n",
        "# === 8. Simpan hasil prediksi ke file CSV ===\n",
        "df_test['predicted_type'] = ['Primary' if label == 0 else 'Secondary' for label in pred_labels]\n",
        "df_test[['article_id', 'dataset_id', 'predicted_type']].to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vqxT25qAuxVg",
        "outputId": "6bb187ed-f8fc-4500-8a53-528e1eec6ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSVQqkvlu9zC",
        "outputId": "8fd5a520-e38a-4e98-a82f-d017588d584a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               article_id                                   dataset_id  \\\n",
            "0    10.1002_2017jc013030        https://doi.org/10.5194/essd-2017-58.   \n",
            "1  10.1002_cssc.202201821       https://doi.org/10.5281/zenodo.7074790   \n",
            "2       10.1002_ece3.3985    https://doi.org/10.1163/1937240X-00002254   \n",
            "3       10.1002_ece3.3985    https://doi.org/10.1017/S0006323199005423   \n",
            "4       10.1002_ece3.3985  https://doi.org/10.1534/genetics.108.100214   \n",
            "\n",
            "                                     teks_dataset_id  \\\n",
            "0  wo databases derived from BGC-Argo float measu...   \n",
            "1  A previous version of this manuscript has been...   \n",
            "2  Bailie ,  D. A. \\n ,  \\n Fitzpatrick ,  S. \\n ...   \n",
            "3  Jennions ,  M. D. \\n , &  \\n Petrie ,  M. \\n  ...   \n",
            "4  Wang ,  J. \\n , &  \\n Santure ,  A. W. \\n  ( 2...   \n",
            "\n",
            "                                        cleaned_teks predicted_type  \n",
            "0  wo databases derived from BGC-Argo float measu...        Primary  \n",
            "1  A previous version of this manuscript has been...        Primary  \n",
            "2  Bailie , D. A. , Fitzpatrick , S. , Connolly ,...        Primary  \n",
            "3  Jennions , M. D. , & Petrie , M. ( 2000 ). Why...        Primary  \n",
            "4  Wang , J. , & Santure , A. W. ( 2009 ). Parent...        Primary  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test['predicted_type'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irLvdxbyz2lr",
        "outputId": "9823988a-efba-46f0-9cd5-05f5f48806c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted_type\n",
            "Primary    193\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}